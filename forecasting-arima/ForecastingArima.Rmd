---
title: "Forecasting - ARIMA models"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

The goal of this phase is to produce the best `univariate time series model` 
for forecasting the return on our stock of choice - Microsoft. For that we will
use family of ARIMA models to find the best performing model.

In previous chapter we explored statistical properties of MSFT returns. 
We will use those findings as an intuition in building and evaluation models 
and explain every step on the way.

This section will we split in the following phases:

- `Data preparation`
- `Stationarity testing`
- `In-sample modeling`
- `Out-of-sample forecast evaluation`
- `Model comparison`

```{r echo=FALSE}
# Importing libraries
suppressMessages(library(quantmod))
suppressMessages(library(forecast))
suppressMessages(library(urca))
```


## Data preparation

Here we're just splitting the sample on the development (“`in-sample`” or 
“`training`”) and on forecast evaluation (“`out-of-sample`” or “`testing`”) 
subsamples.

We'll also look at `ACF` (autocorrelation) plot and `PACF` (partial autocorrelation)
plot in order to check theoretical properties of `AR`, `MA` and `ARMA` models
against our data set.

### Splitting data set

`MSFT returns`:
```{r echo=FALSE}
# Reading data set
MSFT <- read.csv(file = "../data/MSFT.csv", row.names = 1, header = TRUE)

# Converting price series to xts objects
# (in order to work with quantmod library)
MSFT_xts <- xts(MSFT[, 1:5], order.by=as.POSIXct(MSFT$date))
MSFT_xts.retDaily <- periodReturn(MSFT_xts, period = "daily")

# Printing MSFT daily returns
tail(MSFT_xts.retDaily)
```

```{r echo=FALSE}
chartSeries(MSFT_xts.retDaily, theme = "white", up.col="blue", name = "MSFT - Daily returns")
```

Splitting MSFT returns data set into training and testing data set:
```{r echo=FALSE}
# Splitting data set to training set and testing set
train_dataset_range <- '2019::2020'
test_dataset_range <- '2021::'

# Train and test data sets
original_set <- ts(as.numeric(MSFT_xts.retDaily))
training_set <- ts(as.numeric(MSFT_xts.retDaily[train_dataset_range]), frequency = 252, start=c(2019, 1))
testing_set <- ts(as.numeric(MSFT_xts.retDaily[test_dataset_range]), frequency = 252, start=c(2021, 1))

# Data sets size
size <- length(original_set)
training_set_size <- length(training_set)
testing_set_size <- length(testing_set)

print(sprintf("Number of observations in training set: %d (%.2f%%)", training_set_size, 100 * training_set_size / size))
print(sprintf("Number of observations in testing set: %d (%.2f%%)" , testing_set_size, 100 * testing_set_size / size))
```

`Training set`:
```{r echo=FALSE}
head(training_set, 10)
```

`Testing set`:
```{r echo=FALSE}
head(testing_set, 10)
```


### ACF and PACF

Now let's explore ACF and PACF plots to see if we can conclude something
from the plots.

As we're using daily data for returns, we will plot the `10 lags` (to see is 
there is maybe a correlation with lagged term in rage of two weeks - 10 trading
days).

We'll also check for higher number of lags in order to detect if there is maybe 
some `short term stock market cycles`. A cycle can last anywhere from a `few weeks` 
to a `number of years`, depending on the market in question and the time horizon 
at which you look. A day trader using `five-minute` bars may see four or more 
complete cycles per day while, for a real estate investor, a cycle may last 
`18 to 20 years`. Because of that we'll try to plot ACF and PACF with `30 lags`,
which is looking back in past month and a half.

```{r echo=FALSE}
# Prepating plots

# ACF plot
acf_plot_10 <- acf(original_set, lag=10, plot = FALSE)
acf_plot_30 <- acf(original_set, lag=30, plot = FALSE)
acf_plot_250 <- acf(original_set, lag=250, plot = FALSE)

# PACF plot
pacf_plot_10 <- pacf(original_set, lag=10, plot = FALSE)
pacf_plot_30 <- pacf(original_set, lag=30, plot = FALSE)
pacf_plot_250 <- pacf(original_set, lag=250, plot = FALSE)
```

#### ACF and PACF (10 lags):

By looking at the ACF and PACF for 10 lags, we can see that:

- ACF: there are significant serial correlation at lags: 1, 6, 7, 8 and 9.
- PACF: there are significant serial correlation at lags: 6, 7, 8 and 9.

It doesn't look like that ACF and PACF are finite. It looks like that they
are infinite and decaying over time. But it's better to look at the next plots
where we look back for serial correlation in past 30 trading days (30 lags)

```{r echo=FALSE}
# ACF and PACF - 10 lags
plot(acf_plot_10, main = "ACF (10 lags)")
plot(pacf_plot_10, main = "PACF (10 lags)")
```

#### ACF and PACF (30 lags):

By looking at the ACF and PACF for 30 lags, we can see that:

- ACF: there are significant serial correlation at lags: 1, 6, 7, 8, 9, 10, 13,
  14, 15, 16, 21, 22, 25.
- PACF: there are significant serial correlation at lags: 6, 7, 8, 9, 22, 26.

Now it definitely looks like that ACF and PACF are infinite and decaying over 
time.

```{r echo=FALSE}
# ACF and PACF - 30 lags
plot(acf_plot_30, main = "ACF (30 lags)")
plot(pacf_plot_30, main = "PACF (30 lags)")
```

#### ACF and PACF (250 lags):

To be completely sure that ACF and PACF are infinite and decaying over time,
lets use even some higher number of lags, e.g. one year period - `250 lags`.

And on the plot bellow, we can clearly see that its true what we previously
stated for autocorrelation and partial autocorrelation functions.

```{r echo=FALSE}
# ACF and PACF - 250 lags
plot(acf_plot_250, main = "ACF (250 lags)")
plot(pacf_plot_250, main = "PACF (250 lags)")
```

### ARIMA models - Theoretical propertis

In the previous section we investigated ACF and PACF plot for various lags and
if each of the we notices two characteristics - that they are `infinite` and
`decaying over time`.

In the table bellow you will see theoretical ACF and PACF properties that `AR`
(Autoregressive model), `MA` (Moving Average model) and `ARMA` (Autoregressive 
Moving Average model) model have.

|    Process     |                 ACF           |              PACF            |
|----------------|:-----------------------------:|-----------------------------:|
| **AR(p)**      |  Infinite, decaying over time | Finite, 0 for all orders > p |
| **MA(q)**      |  Finite, 0 for all orders > q | Infinite, decaying over time |
| **ARMA(p, q)** |  Infinite, decaying over time | Infinite, decaying over time |


Solely based on the theoretical properties of these models, we expect that
the ARMA models would outperform the rest. But we'll do proper model fitting 
and evaluation using various statistical tests to confirm which model is the
best fit for MSFT daily returns.



## Stationarity testing

In this section we're going to study the stationarity property of the MSFT 
return. We'll perform proper statistical tests to to check if the returns
series is non-stationary.

`Stationary process` is a stochastic process whose unconditional joint 
probability distribution does not change when shifted in time. That means that
`mean` and `variance` also do not change over time. Example of such process is
`random walk`, `random walk with a trend` or `white noise`.

The key properties where stationary and non-stationary processes are different:

- In the long-run stationary series fluctuate around its mean with finite
  variance.
- Random walk tends to deviate in the long run with infinite variance.
- Random walk with drift tends to drift away from the underlying trend
  line.
- Trend stationary series fluctuates around its trend in the long run.


### ETS decomposition

Before we move onto doing proper statistical tests, lets do ETS (Error-Trend-
Seasonal) decomposition We would like to examine if there is maybe some trend 
in the series and to see if the mean is constant. The reason for that is because
for for stationarity testing we will have to specify what is our alternative
hypothesis and what we're testing as the null hypothesis.

```{r echo=FALSE}
# ETS decomposition
original_set <- ts(as.numeric(MSFT_xts.retDaily), frequency = 252, start=c(2019, 1))
autoplot(decompose(original_set))
```

From the ETS decomposition on the plot above we can see that there is `no trend` 
in our time series of returns. We can also notice that there is `no seasonal`
component as well, if there it was, we would be also able to clearly see in in
the ACF plot.


### ADF test

We can perform statistical tests to check if MSFT returns is `unit root process`
(`non-stationary` process), meaning if we could model it using ARMA models.

Augmented Dickey–Fuller test (ADF) tests the null hypothesis that a unit root is
present in a time series sample. The alternative hypothesis is different 
depending on which version of the test is used:

- Ho: `Unit root exists (series is non-stationary)`.
- Ha: We will have to specify the Ha (alternative hypothesis):
  - `None` - zero-mean with no trend (no intercept and no trend) ~ `zero-mean stationarity`.
  - `Drift` - intercept is added ~ `non-zero mean stationarity`.
  - `Trend` - both the trend with an intercept is added ~ `trend stationarity`.
  
As we previously seen, there is no any upwards/downwards trend in returns, so we
won't test against trend stationarity (for our alternative hypothesis Ha). 
As returns are oscillating around zero, we won't test it agains  a non-zero mean 
stationarity.

So for our alternative hypothesis we'll chose parameter `None` - `zero-means stationarity` 
(either an intercept nor a trend is included in the test regression).

```{r echo=FALSE}
adf <- ur.df(original_set, type="none", lags=10, selectlags = "BIC")
summary(adf)
```

By looking at the results of ADF test we can see that value for our `test-statistics`
is `-7.3146 ` and the critical values for the `rejection region`:

- `90% confidence level`: `-1.62`
- `95% confidence level`: `-1.95`
- `999% confidence level`: `-2.58`

As we're looking at the left tail, our test statistics  is inside the 
`reject region`, which means we will `reject the Ho`.
That means that MSFT returns have `zero-mean stationary` property.


### ERS test

Elliott, Rothenberg and Stock Unit Root Test (ERS) is a modification of ADF test.
For this null hypothesis is the same - that it's a unit root process.

By looking at the results of ERS test we can see that value for our `test-statistics`
is `-2.3161 ` and the critical values for the `rejection region`:

- `90% confidence level`: `-1.62`
- `95% confidence level`: `-1.94`
- `99% confidence level`: `-2.57`


As we're looking at the left tail, our test statistics is inside the 
`reject region` with a confidence of `95%`, which means we will `reject the Ho`.
That means that MSFT returns is a `stationary` process.

```{r echo=FALSE}
ers <- ur.ers(original_set, type = "DF-GLS", model = "constant", lag.max = 10)
summary(ers)
```

### KPSS test

Kwiatkowski–Phillips–Schmidt–Shin (KPSS)  is another stationarity tests, where
the null hypothesis that an observable time series is stationary (different 
copared to the last two tests).

By looking at the results of KPSS test we can see that value for our `test-statistics`
is `0.0452  ` and the critical values for the `rejection region`:

- `90% confidence level`: `0.347`
- `95% confidence level`: `0.463`
- `97.2% confidence level`: `0.574`
- `99% confidence level`: `0.739`

As we're looking at the right tail, our test statistics is `outside`  of the 
`reject region` for all levels of confidence. So we don't reject the null
hypothesis, meaning that the returns are exibiting `stationary property`

```{r echo=FALSE}
kpss <- ur.kpss(original_set, type = "mu", lags = "short", use.lag = 10)
summary(kpss)
```


### Stationarity overview

For all of the statistical test we got the the `MSFT returns are stationary`.
